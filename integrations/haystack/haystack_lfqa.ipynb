{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaazjsqpV0dB"
   },
   "source": [
    "# Abstractive QA with LFQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BrhVtyhV7wg"
   },
   "source": [
    "[*Find the full article version of this notebook here*](https://www.pinecone.io/learn/haystack-lfqa/)\n",
    "\n",
    "We have seen incredible breakthroughs in Natural Language Processing (NLP) in the last several years. [Question Answering](https://www.pinecone.io/learn/question-answering/) (QA) systems that leverage popular language models such as BERT, ROBERTA, etc., can now easily answer questions from a given context with great precision. These QA systems accept a question, locate the most relevant document passages containing answers from a document store, and extract or generate the most likely answer.\n",
    "\n",
    "Recent studies have focused on more advanced QA systems such as Long-Form Question Answering (LFQA) systems that can generate multi-sentenced abstractive (generated) answers to open-ended questions. It works by searching massive document stores for documents containing relevant information and then using this information to compose an accurate multi-sentence answer synthetically. The relevant documents give larger context for generating original, abstractive long-form answers.\n",
    "\n",
    "At present, searching for information on a topic is painstaking. For instance, we might multiple queries on Google or any other search engine to pull snippets of information from several sources. LFQA simplifies this by pulling info from several sources and compressing them into a single, human-like answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SfduZPFWBco"
   },
   "source": [
    "## Using Haystack for LFQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VeNnTi8qWLvW"
   },
   "source": [
    "We will use [Haystack](https://www.pinecone.io/docs/integrations/haystack/) to build a LFQA system. Three main components are needed to build a LFQA pipeline in Haystack: *DocumentStore, Retriever, and Generator.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJj4Q8ZtWTBQ"
   },
   "source": [
    "### DocumentStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccPXUHRsWWZO"
   },
   "source": [
    "As the name suggests, the document store is where all our documents are stored. Haystack has different document stores we can use for various use cases. For LFQA, we will use a dense/embedding-based retriever, but it is possible to use traditional methods such as TF–IDF and BM25. So, we need a vector-optimized document store to hold embedding vectors that represent our documents. We will use the *PineconeDocumentStore*, now available in Haystack starting from [version 1.3.0](https://github.com/deepset-ai/haystack/releases/tag/v1.3.0). We could have easily used any other vector-optimized document store such as *FAISSDocumentStore*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKNb2ex8Wcot"
   },
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVPvzMkcWnz4"
   },
   "source": [
    "The QA system needs information relevant to the query to generate an answer to the question. So, we need to retrieve the documents containing relevant information from the document store. The retriever’s job is to find the best candidates by computing the similarity between the question and the document embeddings. The final answer is generated based on the best candidates.\n",
    "\n",
    "We will use Haystack’s *EmbeddingRetriever* in our LFQA pipeline. It works by first generating the query embedding using a language model and then computing the dot product or cosine similarity between the document embeddings in the document store. Then, the top-k most relevant documents are retrieved. We will use a SentenceTransformer model fine-tuned for the query/document matching task as the retriever.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vR8S2z-oW45K"
   },
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gTowxNbW2mf"
   },
   "source": [
    "We will use ELI5 BART for the generator - a sequence-to-sequence model trained using the ‘Explain Like I’m 5’ (ELI5) dataset. Sequence-to-Sequence models can take a text sequence as input and produce a different text sequence as the output.\n",
    "\n",
    "The input to the ELI5 BART model is a single string which is a concatenation of the query and the relevant documents providing the context for the answer. The documents are separated by a special token &lt;P>, so the input string will look as follows:\n",
    "\n",
    ">question: What is a sonic boom? context: &lt;P> A sonic boom is a sound associated with shock waves created when an object travels through the air faster than the speed of sound. &lt;P> Sonic booms generate enormous amounts of sound energy, sounding similar to an explosion or a thunderclap to the human ear. &lt;P> Sonic booms due to large supersonic aircraft can be particularly loud and startling, tend to awaken people, and may cause minor damage to some structures. This led to prohibition of routine supersonic flight overland.\n",
    "\n",
    "We will use Haystack’s *Seq2SeqGenerator* - a generic sequence-to-sequence generator based on HuggingFace's transformers library, to initialize the BART model. When using *Seq2SeqGenerator* the concatenation process above is automatically handled by the haystack and transformers library. The generator will compose a paragraph-long answer based on the relevant context documents.\n",
    "\n",
    "More detail on how the ELI5 dataset was built is available [here](https://arxiv.org/abs/1907.09190) and how ELI5 BART model was trained is available [here](https://yjernite.github.io/lfqa.html).\n",
    "\n",
    "\n",
    "Now let's build our LFQA system using Haystack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-vHGLnqX4Pn"
   },
   "source": [
    "## Preparing the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ts1F6fu3dMZ",
    "outputId": "c36bf177-974f-4a67-9e86-ffadd6366734"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 11 13:33:22 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P0    59W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM...  Off  | 00000000:00:05.0 Off |                    0 |\n",
      "| N/A   37C    P0    56W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Make sure you have a GPU running to speed up things.\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GThnZdT63k2s"
   },
   "source": [
    "Install the required libraries and their dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pinecone-client\n",
    "!pip install -U 'farm-haystack[pinecone]'>=1.8.0\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxpan1D1a2gd"
   },
   "source": [
    "## Initializing the PineconeDocumentStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0x_P2jAbvPu"
   },
   "source": [
    "We need an API key to use the PineconeDocumentStore in Haystack (you can sign up for free [here](https://app.pinecone.io/) and get an API key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MyzvG346a5Z3",
    "outputId": "2664c8f1-b9d3-4aa3-c08b-b3930d985db8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n",
      "INFO - haystack.document_stores.pinecone -  Index statistics: name: haystack-lfqa, embedding dimensions: 768, record count: 0\n"
     ]
    }
   ],
   "source": [
    "from haystack.document_stores import PineconeDocumentStore\n",
    "\n",
    "document_store = PineconeDocumentStore(\n",
    "    api_key='<<YOUR_API_KEY>>',\n",
    "    index='haystack-lfqa',\n",
    "    similarity=\"cosine\",\n",
    "    embedding_dim=768\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Rv9J3lDGt4Kp",
    "outputId": "de136e95-352b-4ce7-fd18-1816c9c01a91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cosine'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_store.metric_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gLRKiIN_cfob",
    "outputId": "bceff4e1-a878-4318-b2fc-26418f3f4cec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_store.get_document_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z8c62cWvmLdX",
    "outputId": "fa47853f-2efc-465b-85b4-ecb79123c6c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_store.get_embedding_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GV_618Q1g-fs"
   },
   "source": [
    "The above code is all we need to initialize a Pinecone document store with Haystack. It will either create a Pinecone index named ```haystack-lfqa``` if it is not already there or connect to an existing index with the same name. The embedding dimension is set to 768 as the SentenceTransformer model we use to encode queries and documents outputs a vector with 768 dimensions. We also set the similarity metric to cosine as this particular model was trained to be used with cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaCnEvLFg-lg"
   },
   "source": [
    "## Preparing and Indexing Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XweCiHNnjQRn"
   },
   "source": [
    "We will use the Wiki Snippets dataset, containing over 17 million passages from Wikipedia, as our source documents. But for this demo, we will use only fifty thousand passages which contains 'History' in the 'section_title' column as indexing the whole dataset will take a lot of time. But feel free to use the entire dataset if you wish. Pinecone vector database can easily handle millions of documents for you. This dataset is available on HuggingFace, so we can use the HuggingFace dataset library to load the dataset quickly and filter the historical passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X6PBceFOi-uD",
    "outputId": "78eacb72-b7b4-4625-92db-72e763ea904c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - datasets.builder -  Using custom data configuration default\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<datasets.iterable_dataset.IterableDataset at 0x7f07d68a1910>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wiki_data = load_dataset(\n",
    "    'vblagoje/wikipedia_snippets_streamed',\n",
    "    split='train',\n",
    "    streaming=True\n",
    ")\n",
    "wiki_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkpdlVGWgajI"
   },
   "source": [
    "We are loading the dataset in the streaming mode so that we don't have to wait for the whole dataset to download (which is over 9GB). Instead, we access the data when we iterate through the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5v7HrmxfRa1",
    "outputId": "b3d08653-07d5-4885-c2a2-1fd906480e84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wiki_id': 'Q7593707',\n",
       " 'start_paragraph': 2,\n",
       " 'start_character': 0,\n",
       " 'end_paragraph': 6,\n",
       " 'end_character': 511,\n",
       " 'article_title': \"St John the Baptist's Church, Atherton\",\n",
       " 'section_title': 'History',\n",
       " 'passage_text': \"St John the Baptist's Church, Atherton History There have been three chapels or churches on the site of St John the Baptist parish church. The first chapel at Chowbent was built in 1645 by John Atherton as a chapel of ease of Leigh Parish Church. It was sometimes referred to as the Old Bent Chapel. It was not consecrated and used by the Presbyterians as well as the Vicar of Leigh. In 1721 Lord of the manor Richard Atherton expelled the dissenters who subsequently built Chowbent Chapel. The first chapel was consecrated in 1723 by the Bishop of Sodor and\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the contents of a single document in the dataset\n",
    "next(iter(wiki_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LyIMo-tS30KY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datasets.iterable_dataset.IterableDataset at 0x7f07d686df10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter only documents with History as section_title\n",
    "history = wiki_data.filter(lambda d: d['section_title'].startswith('History'))\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXP4zUh3phWv"
   },
   "source": [
    "To index the documents, we first create Haystack Document objects containing the content and metadata for each document. We are iterating through the filtered dataset and adding the documents to the document store every time when ten thousand Document objects are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209,
     "referenced_widgets": [
      "1be56b37dc0b4f118d7c0126134fb32f",
      "511108b045cc409bb0a6e773d4a478a3",
      "cb68d17fdeb8494f8f54932f1587a854",
      "c124328790bc467e8c56b5ac2ccda3c2",
      "f3ab5d5c30f54e94a22f19c0ce1a0578",
      "316234cf0a0941df9c213d023dd5a907",
      "77fd6e6722034dfd8502be461d80ed25",
      "0d909e28a64f41e99b22bb6f48d1ec7e",
      "7a5bde72e7db4ebabb71e3bf13c8e868",
      "cbdd315059c7408cac7f5df957e6d6a3",
      "346859a134ef4a2abb12d31a5f5628c9",
      "9f04d126dd814e459387682810a6fff0",
      "789becf3189f4a65baa7ec39291c1b4f",
      "0d6b3830f4d64aa2be699d4fd23233c0",
      "7af8972d76a845cd89eb27d2ec611045",
      "56baeeb4a0b74d70949b290a7fb3f4df",
      "281dcc3255af4148919d28b7746ff753",
      "c41267d378a444f0aa2b5a67e9e70f85",
      "e2f5f966cbb749a08c103125192d2247",
      "a527d45904084b66b140247228047b87",
      "87b449bec4954a85b8f8eb5b0a18fa35",
      "a4fed485526c4bd78a19e25c69f526ce",
      "47372e7392654103982ee952867da0c9",
      "119af7d915134140ae0bff70cd5e868a",
      "065d4244d0c84f40b16f6b8f7ae58b4d",
      "e2021cd96207402aa2f924628a45f4db",
      "594cd44ef8e8450da85d5394ba7d4b2b",
      "1c1a35e470a1469da85cb99e2e1f61b6",
      "14613db308154dd090e09116f53d2b8e",
      "27ef2ff85b184a95b1a82cddf2e90fcd",
      "97ba3ec1557d44438e218827a08e4157",
      "6ec46423a5f3458185a1de1952661bde",
      "0ac7dea057fe4d678492bc558bd8d307",
      "2fef130ad770405cb8771c91c894bd35",
      "1c9bde7632084c41bd1ae036e90efcc0",
      "79b91cf2e5074d2aaf34c37e2edcc909",
      "2c55a4ed10bc45a89b5d887db25e214b",
      "bb725479be154aa98b2743af932602f1",
      "b5c9cb64dfe144b7971d620e59562a2b",
      "ca704f4c7c504a849df1a18c8dfa3706",
      "9e7b4ddf6ded4e7d94b23df0e54235f5",
      "de88edce71144b1d905a0be04ddd3112",
      "e8c8136072bb4f26bac65d9099050163",
      "23bb5a4253b2494b8fa08f93c56a7aee",
      "bf2161ac3ea148a8b4bf9db80e40f3f7",
      "3e3e83ca5cd249328496a75d07c6fc04",
      "9fe4900cc5754e569999cb0c4c4fffbc",
      "d62201032b714e578f0653426f47d54b",
      "a10d1b19290149ad9c55fbf775230ea0",
      "523d9e2539a24c43b164b5339308efce",
      "f68efee5018147ab8e7c67a409b6ec0c",
      "ecf510e4b05340679572ae3f80c308d1",
      "4063826a016b4e968f916df2faa48b93",
      "97215420357a4f46924ca3da3bcfc80f",
      "6da294ba33e643538c5735168e5342e4",
      "57bdd2058507427194ae3aa783905e14",
      "b3886798f1364ee9a5dba68c1812cb28",
      "ebbd362d93b04c7f872ecdde663f8df7",
      "8ff6027389fd4a81be4e33e64bd3e454",
      "bf6bbea0481046c5abe7182f20a0fb84",
      "90d65ce539d24db0adc3f17c5d0e691e",
      "40d5053fb6244053ab8bb818c7e8c061",
      "8870e93742054ac09c5bd63bc8fdac96",
      "f93da7ce09174b3d809abe8f126b997c",
      "8a7c4c5314d442f6ab479413c6c6d460",
      "811d3bab24574fc6a5f024771f7eeee9"
     ]
    },
    "id": "v1i4jTHAnVyq",
    "outputId": "7199d974-f09c-458d-e337-3c1a2f834816"
   },
   "outputs": [],
   "source": [
    "from haystack import Document\n",
    "from tqdm.auto import tqdm  # progress bar\n",
    "\n",
    "total_doc_count = 50000\n",
    "batch_size = 10000\n",
    "\n",
    "counter = 0\n",
    "docs = []\n",
    "for d in tqdm(history, total=total_doc_count):\n",
    "    # create haystack document object with text content and doc metadata\n",
    "    doc = Document(\n",
    "        content=d[\"passage_text\"],\n",
    "        meta={\n",
    "            \"article_title\": d[\"article_title\"],\n",
    "            'section_title': d['section_title']\n",
    "        }\n",
    "    )\n",
    "    docs.append(doc)\n",
    "    counter += 1\n",
    "    if counter % batch_size == 0:\n",
    "        # writing docs everytime 10k docs are reached\n",
    "        document_store.write_documents(docs)\n",
    "        docs.clear()\n",
    "    if counter == total_doc_count:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9GRIObeVt15r",
    "outputId": "c49cfe29-55b0-48e8-d26e-daca7e4a1b90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49995"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_store.get_document_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_store.get_embedding_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SWIkD4trMtv"
   },
   "source": [
    "Now all our documents are added to the *PineconeDocumentStore*. Next, we need to initialize the second component in our LFQA system - the Retriever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fk_LOW8quyzA"
   },
   "source": [
    "## Initializing the Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W32cUk2JvC8q"
   },
   "source": [
    "We will use Haystack's *EmbeddingRetriever* with a SentenceTransformer model trained based on Microsoft's MPNet. This model performs quite well for comparing the similarity between queries and documents. We can use the retriever to easily compute and update the embeddings for all the documents in the document store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ESKK9I1Vkk73",
    "outputId": "b5f93a43-afdc-40f1-ff22-aacbc1cd2bf8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# confirm GPU is available (if using CPU this step will be slower)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BZmgZ3gzdM8M",
    "outputId": "92452b24-c90c-4d07-e0e4-2acd4d884c46"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CUDA:0, CUDA:1\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 2\n",
      "INFO - haystack.retriever.dense -  Init retriever using embeddings of model flax-sentence-embeddings/all_datasets_v3_mpnet-base\n",
      "WARNING - haystack.nodes.retriever._embedding_encoder -  You are using a Sentence Transformer with the None function. We recommend using cosine instead. This can be set when initializing the DocumentStore\n"
     ]
    }
   ],
   "source": [
    "from haystack.nodes.retriever import EmbeddingRetriever\n",
    "\n",
    "retriever = EmbeddingRetriever(\n",
    "    document_store=document_store,\n",
    "    embedding_model=\"flax-sentence-embeddings/all_datasets_v3_mpnet-base\",\n",
    "    model_format=\"sentence_transformers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJhwAHGvx2Ei"
   },
   "source": [
    "It will take some time to compute all the embeddings and update the index. If you have access to a GPU, it will significantly speed up the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3G9n_DghSrM_"
   },
   "source": [
    "## Creating and Upserting Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "gBnWiSwpw_8n"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.document_stores.pinecone -  Updating embeddings for 49995 docs...\n"
     ]
    }
   ],
   "source": [
    "document_store.update_embeddings(\n",
    "    retriever,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSvBuUfIxXso"
   },
   "source": [
    "The embeddings are updated to the document store, and our retriever is now ready. Let's test the retriever before we use it in the LFQA pipeline. We can test queries with the retriever by loading it into a *DocumentSearchPipeline*. Keep in mind we are only using fifty thousand passages from the Wiki Snippets dataset, so it is likely that documents containing relevant information for our exact queries are not in the document store. You can always test whether relevant documents are returned by running some queries on *DocumentSearchPipeline*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 570,
     "referenced_widgets": [
      "b7765b9f9b504a8484b8ca74d1edd767",
      "a786984ca5f74c679df72032ebceb6e4",
      "208138a54b2e4b4ea827d57437b0c9d7",
      "24345b32708e40b6b407e80172d7733b",
      "c86c822e93c84e77a7e5f88fd1d43ed1",
      "b842d362bf6f444398e95c38866c8a12",
      "df7a309bb49b47089021523c2534a362",
      "87c6ab965ad3452c913caf5f9b6d38bf",
      "e6360bc79b6143f49d626c1abea007a3",
      "a4500f3043d5404b9b04da7aeea8b23a",
      "4fc7bf5075cb4303904e5a2e9eadffed"
     ]
    },
    "id": "5AY24GCFyObj",
    "outputId": "ebe5df3e-ad4b-4435-d85d-5024665804f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: When was the first electric power system built?\n",
      "\n",
      "{   'content': 'Electric power system History In 1881, two electricians built '\n",
      "               \"the world's first power system at Godalming in England. It was \"\n",
      "               'powered by two waterwheels and produced an alternating current '\n",
      "               'that in turn supplied seven Siemens arc lamps at 250 volts and '\n",
      "               '34 incandescent lamps at 40 volts. However, supply to the '\n",
      "               'lamps was intermittent and in 1882 Thomas Edison and his '\n",
      "               'company, The Edison Electric Light Company, developed the '\n",
      "               'first steam-powered electric power station on Pearl Street in '\n",
      "               'New York City. The Pearl Street Station initially powered '\n",
      "               'around 3,000 lamps for 59 customers. The power station '\n",
      "               'generated direct current and',\n",
      "    'name': None}\n",
      "\n",
      "{   'content': 'by a coal burning steam engine, and it started generating '\n",
      "               'electricity on September 4, 1882, serving an initial load of '\n",
      "               '400 incandescent lamps used by 85 customers located within '\n",
      "               'about 2 miles (3.2\\xa0km) of the station. \\n'\n",
      "               'However, with the advent of AC, there came the use of '\n",
      "               'transformers to convert the generated power to a much higher '\n",
      "               'voltage for transmission allowed the power plants and users to '\n",
      "               'be separated by hundreds of miles if needed. The high voltage '\n",
      "               'could then use transformers to obtain lower voltages for final '\n",
      "               'use. Single point failures were minimized in the plant design. '\n",
      "               'The AC',\n",
      "    'name': None}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from haystack.pipelines import DocumentSearchPipeline\n",
    "from haystack.utils import print_documents\n",
    "\n",
    "search_pipe = DocumentSearchPipeline(retriever)\n",
    "result = search_pipe.run(\n",
    "    query=\"When was the first electric power system built?\",\n",
    "    params={\"Retriever\": {\"top_k\": 2}}\n",
    ")\n",
    "\n",
    "print_documents(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0_Camne0E0n"
   },
   "source": [
    "As you can see, the retriever can find relevant documents from the document store. Now let's initialize the third compontent in our LFQA system - the Generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkZ3G-Lj2-RS"
   },
   "source": [
    "## Initializing the Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYBeibfQ3JaU"
   },
   "source": [
    "For the generator we will load Haystack’s generic *Seq2SeqGenerator* with a model trained specifically for LFQA. We could use bart_lfqa by [vblagoje](https://huggingface.co/vblagoje) or bart_eli5 by [yjernite](https://huggingface.co/yjernite), both models performs quite well. bart_lfqa was trained with a newer ELI5 dataset, so we will go with that. For more details about the new ELI5 dataset, please refer to this [article](https://towardsdatascience.com/long-form-qa-beyond-eli5-an-updated-dataset-and-approach-319cb841aabb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vnVNXAH1R2c",
    "outputId": "7b8fe51e-beba-49ed-cd0d-4e809cb21eb2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CUDA\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "from haystack.nodes import Seq2SeqGenerator\n",
    "\n",
    "generator = Seq2SeqGenerator(model_name_or_path=\"vblagoje/bart_lfqa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTmHQvKP8s4v"
   },
   "source": [
    "## Initializing a Generative QA Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtVBvB0t8zjU"
   },
   "source": [
    "Finally, we need to add the retriever and generator to Haystack's *GenerativeQAPipeline*, a ready-made pipeline for generative QA task. The *GenerativeQAPipeline*, as you might expect, combines the retriever with the generator to produce answers to our questions, and it is the primary interface for communicating with our LFQA system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "osloQQS09bgR",
    "outputId": "a3e71759-0906-46b2-e3a4-06e178fefe0b"
   },
   "outputs": [],
   "source": [
    "from haystack.pipelines import GenerativeQAPipeline\n",
    "\n",
    "pipe = GenerativeQAPipeline(generator, retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-nmJGH3KxhO"
   },
   "source": [
    "## Asking Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyqgGp7WApd3"
   },
   "source": [
    "Now let's run some queries in our LFQA system. When queying we can specificy the number of documents we want the retriever to retrieve and the number of answers the generator to produce. The final answers will be generated based on the documents retrieved from the document store.\n",
    "\n",
    "The code below is what you need to run queries in the LFQA system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208,
     "referenced_widgets": [
      "46857a1b41e64d57a470fa1322e32439",
      "597aff7a0a0345d6b22e832a45b1cb79",
      "4be5cd7c4fae48bbbd783a1280d9a0b9",
      "77d28a7c299a48899f09b968dcbc2861",
      "7b38780969474fbab6b58f3d4e147e8a",
      "4849f113a5c84f5986dde99158877b06",
      "52e7f54d9a774061954bb0dbef50cf52",
      "362945e9feaa4272a03a20fdfd0d6dd7",
      "2ead7a30cb59412d9aeae71c36a41d8f",
      "2d8e03f94581474bb3e4c9b078b64d6a",
      "5ef36baa18a348c481566fedfa4d49a2"
     ]
    },
    "id": "QrSrcSQFK4AH",
    "outputId": "94b91785-751a-4a75-c6e6-30c9ad030da2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b7cbd392aa491ea33384bd9a9e2348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'what was the war of currents?',\n",
       " 'answers': [<Answer {'answer': \"The War of Currents was the rivalry between Thomas Edison and George Westinghouse's companies over which form of transmission (direct or alternating current) was superior.\", 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_id': None, 'meta': {'doc_ids': ['3a43249b33b1435e94ef9b22f01989b6', '54f12cf9010626d589b22712a1547983', 'ef560b97430a71ba9ff193062d6a9d0b'], 'doc_scores': [0.5010574158160604, 0.5009853666918446, 0.5009245203338678], 'content': ['consultant at the Westinghouse Electric & Manufacturing Company\\'s Pittsburgh labs.\\nBy 1888, the electric power industry was flourishing, and power companies had built thousands of power systems (both direct and alternating current) in the United States and Europe. These networks were effectively dedicated to providing electric lighting. During this time the rivalry between Thomas Edison and George Westinghouse\\'s companies had grown into a propaganda campaign over which form of transmission (direct or alternating current) was superior, a series of events known as the \"War of Currents\". In 1891, Westinghouse installed the first major power system that was designed to drive a', 'of the British administration a favorite route for the smuggling of slaves.', 'of migration began, this state of affairs sometimes led to international incidents, with countries of origin refusing to recognize the new nationalities of natives who had migrated, and when possible, conscripting natives who had naturalized as citizens of another country into military service. The most notable example was the War of 1812, triggered by British impressment of American seamen who were alleged to be British subjects into naval service.\\nIn the aftermath of the 1867 Fenian Rising, Irish-Americans who had gone to Ireland to participate in the uprising and were caught were charged with treason, as the British authorities considered them'], 'titles': ['', '', '']}}>],\n",
       " 'documents': [<Document: {'content': 'consultant at the Westinghouse Electric & Manufacturing Company\\'s Pittsburgh labs.\\nBy 1888, the electric power industry was flourishing, and power companies had built thousands of power systems (both direct and alternating current) in the United States and Europe. These networks were effectively dedicated to providing electric lighting. During this time the rivalry between Thomas Edison and George Westinghouse\\'s companies had grown into a propaganda campaign over which form of transmission (direct or alternating current) was superior, a series of events known as the \"War of Currents\". In 1891, Westinghouse installed the first major power system that was designed to drive a', 'content_type': 'text', 'score': 0.5010574158160604, 'meta': {'article_title': 'Electric power system', 'section_title': 'History'}, 'embedding': None, 'id': '3a43249b33b1435e94ef9b22f01989b6'}>,\n",
       "  <Document: {'content': 'of the British administration a favorite route for the smuggling of slaves.', 'content_type': 'text', 'score': 0.5009853666918446, 'meta': {'article_title': 'Muri, Nigeria', 'section_title': 'History'}, 'embedding': None, 'id': '54f12cf9010626d589b22712a1547983'}>,\n",
       "  <Document: {'content': 'of migration began, this state of affairs sometimes led to international incidents, with countries of origin refusing to recognize the new nationalities of natives who had migrated, and when possible, conscripting natives who had naturalized as citizens of another country into military service. The most notable example was the War of 1812, triggered by British impressment of American seamen who were alleged to be British subjects into naval service.\\nIn the aftermath of the 1867 Fenian Rising, Irish-Americans who had gone to Ireland to participate in the uprising and were caught were charged with treason, as the British authorities considered them', 'content_type': 'text', 'score': 0.5009245203338678, 'meta': {'article_title': 'Multiple citizenship', 'section_title': 'History'}, 'embedding': None, 'id': 'ef560b97430a71ba9ff193062d6a9d0b'}>],\n",
       " 'root_node': 'Query',\n",
       " 'params': {'Retriever': {'top_k': 3}, 'Generator': {'top_k': 1}},\n",
       " 'node_id': 'Generator'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pipe.run(\n",
    "        query=\"what was the war of currents?\",\n",
    "        params={\n",
    "            \"Retriever\": {\"top_k\": 3},\n",
    "            \"Generator\": {\"top_k\": 1}\n",
    "        })\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clean up the output using Haystack's `print_answers` util."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6658406ffd403f9ab7929cd67e3b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: what was the war of currents?\n",
      "Answers:\n",
      "[   {   'answer': 'The War of Currents was the rivalry between Thomas Edison '\n",
      "                  \"and George Westinghouse's companies over which form of \"\n",
      "                  'transmission (direct or alternating current) was superior.'}]\n"
     ]
    }
   ],
   "source": [
    "from haystack.utils import print_answers\n",
    "\n",
    "result = pipe.run(\n",
    "        query=\"what was the war of currents?\",\n",
    "        params={\n",
    "            \"Retriever\": {\"top_k\": 3},\n",
    "            \"Generator\": {\"top_k\": 1}\n",
    "        })\n",
    "\n",
    "print_answers(result, details=\"minimum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer here is good although there is not too much detail. When we find an answer is either not good or lacking detail there can be two combining factors for this:\n",
    "\n",
    "* The generator model has not been trained on data that includes information about the *\"war on currents\"* and so it has not *memorized* this information within it's model weights.\n",
    "\n",
    "* We have not returned any contexts that contain the answer, so the generator has no reliable external sources of information.\n",
    "\n",
    "If neither of these conditions are satisfied, the generator cannot produce a factually correct answer. However, in our case we are returning some good external context. We can try and return more detail by increasing the number of contexts retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ffe7e2a750545f4952ca04652fdb1e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: what was the war of currents?\n",
      "Answers:\n",
      "[   {   'answer': 'The \"War of Currents\" was a rivalry between Edison and '\n",
      "                  'George Westinghouse over the use of alternating current '\n",
      "                  '(AC) in power transmission. In 1891, the first major power '\n",
      "                  'system that was designed to drive a generator with '\n",
      "                  'alternating current was installed in the United States. '\n",
      "                  'This system was called the \"Westinghouse Electric System\" '\n",
      "                  '(WES) and was the first to use alternating current to power '\n",
      "                  'a generator. By the end of the 19th century, the WES system '\n",
      "                  'had become the dominant form of power transmission in the '\n",
      "                  'U.S. and the rest of the world. However, there was still a '\n",
      "                  'lot of competition between the two power companies over '\n",
      "                  'which form of transmission was better. In the early 1900s, '\n",
      "                  'the two companies were still at odds over whether AC or DC '\n",
      "                  'should be used to power the power grid. In 1902, the '\n",
      "                  'Supreme Court ruled that DC was superior to AC, and in '\n",
      "                  '1903, Congress passed the Emancipation Proclamation, which '\n",
      "                  'prohibited'}]\n"
     ]
    }
   ],
   "source": [
    "result = pipe.run(\n",
    "        query=\"what was the war of currents?\",\n",
    "        params={\n",
    "            \"Retriever\": {\"top_k\": 10},\n",
    "            \"Generator\": {\"top_k\": 1}\n",
    "        })\n",
    "\n",
    "print_answers(result, details=\"minimum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're seeing much more info. Some of it rambles but for the most part it is relevant. We can also compare these results to generator created answer *without* any context by querying the generator directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: what was the war of currents?\n",
      "Answers:\n",
      "[{'answer': 'I\\'m not sure what you mean by \"war\".'}]\n"
     ]
    }
   ],
   "source": [
    "result = generator.predict(\n",
    "    query=\"what was the war of currents?\",\n",
    "    documents=[Document(content=\"\")],\n",
    "    top_k=1\n",
    ")\n",
    "\n",
    "print_answers(result, details=\"minimum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the retrieved contexts are important. Although this isn't always the case, for example if we ask a more well-known question..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: who was the first person on the moon?\n",
      "Answers:\n",
      "[{'answer': 'The first man to walk on the moon was Neil Armstrong.'}]\n"
     ]
    }
   ],
   "source": [
    "result = generator.predict(\n",
    "    query=\"who was the first person on the moon?\",\n",
    "    documents=[Document(content=\"\")],\n",
    "    top_k=1\n",
    ")\n",
    "\n",
    "print_answers(result, details=\"minimum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this type of general knowledge, the generator model is able to pull the answer directly from it's own *\"memory\"*, eg the model weights optimized during training, where it will have been given training data containing this information. Larger models have a larger memory, but when asking more specific questions (like our question about the war on currents) we rarely return good answers without an external data source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some more questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549fa91e8ccc4bfab07bbf392ca16ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: when was the first electric power system built?\n",
      "Answers:\n",
      "[   {   'answer': 'The first steam powered power station was built in 1881 at '\n",
      "                  'Godalming in England. It was powered by two waterwheels and '\n",
      "                  'produced an alternating current that in turn supplied seven '\n",
      "                  'Siemens arc lamps at 250 volts and 34 incandescent lamps at '\n",
      "                  '40 volts. The power station initially powered around 3,000 '\n",
      "                  'lamps for 59 customers. By 1888, the power companies had '\n",
      "                  'built thousands of power systems (both direct and '\n",
      "                  'alternating current) in the United States and Europe.'}]\n"
     ]
    }
   ],
   "source": [
    "result = pipe.run(\n",
    "        query=\"when was the first electric power system built?\",\n",
    "        params={\n",
    "            \"Retriever\": {\"top_k\": 3},\n",
    "            \"Generator\": {\"top_k\": 1}\n",
    "        })\n",
    "\n",
    "print_answers(result, details=\"minimum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm the correctness of this answer by checking the contexts that this answer has been built from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electric power system History In 1881, two electricians built the world's first power system at Godalming in England. It was powered by two waterwheels and produced an alternating current that in turn supplied seven Siemens arc lamps at 250 volts and 34 incandescent lamps at 40 volts. However, supply to the lamps was intermittent and in 1882 Thomas Edison and his company, The Edison Electric Light Company, developed the first steam-powered electric power station on Pearl Street in New York City. The Pearl Street Station initially powered around 3,000 lamps for 59 customers. The power station generated direct current and\n",
      "---\n",
      "consultant at the Westinghouse Electric & Manufacturing Company's Pittsburgh labs.\n",
      "By 1888, the electric power industry was flourishing, and power companies had built thousands of power systems (both direct and alternating current) in the United States and Europe. These networks were effectively dedicated to providing electric lighting. During this time the rivalry between Thomas Edison and George Westinghouse's companies had grown into a propaganda campaign over which form of transmission (direct or alternating current) was superior, a series of events known as the \"War of Currents\". In 1891, Westinghouse installed the first major power system that was designed to drive a\n",
      "---\n",
      "by a coal burning steam engine, and it started generating electricity on September 4, 1882, serving an initial load of 400 incandescent lamps used by 85 customers located within about 2 miles (3.2 km) of the station. \n",
      "However, with the advent of AC, there came the use of transformers to convert the generated power to a much higher voltage for transmission allowed the power plants and users to be separated by hundreds of miles if needed. The high voltage could then use transformers to obtain lower voltages for final use. Single point failures were minimized in the plant design. The AC\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for doc in result['documents']:\n",
    "    print(doc.content, end='\\n---\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases the generator will generate a false answer if it is asked about a topic and does not recieve any relevant contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384,
     "referenced_widgets": [
      "ba55863ada104dfa98df32656333f6e3",
      "b2bbb6d1628d4814941739376e692a6d",
      "2b46ae746958477bb87e9c4568d26460",
      "7da0ed678d5e436ba72f1cb7b2cf5f86",
      "885e3f0b9b5c49f596b041f61ada4ad8",
      "73aa003466874537ac82b918e1f66f4e",
      "3d00e3275de64bf39de43cfce1031a7d",
      "5b65f078bbd54c1abba9ff3e4a1b1107",
      "67b0f85821e24542bffdb6614758056a",
      "8b3bb908bb5d41f09f4d6512eb053873",
      "da4a7275056c415a8f5225b1d987b984"
     ]
    },
    "id": "QudhKU-9QX8N",
    "outputId": "e862a651-913d-4298-b9be-5e0ac3686e5d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7561269435664a2eb520e7d6d49ddc66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: where did COVID-19 originate?\n",
      "Answers:\n",
      "[   {   'answer': \"COVID-19 isn't a virus, it's a bacterium. It's a virus that \"\n",
      "                  \"infects bacteria. There's no way to know where it came \"\n",
      "                  'from.'}]\n"
     ]
    }
   ],
   "source": [
    "result = pipe.run(\n",
    "        query=\"where did COVID-19 originate?\",\n",
    "        params={\n",
    "            \"Retriever\": {\"top_k\": 3},\n",
    "            \"Generator\": {\"top_k\": 1}\n",
    "        })\n",
    "\n",
    "print_answers(result, details=\"minimum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdjDZP-KXtiL"
   },
   "source": [
    "This is one drawback of the LFQA pipeline, although this can be mitigated to an extent by implementing thresholds on answer confidence scores, and including the sources behind any generated answers. Let's finish with a few final questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f035b836724442e6bdb1c988ba17c3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: what was NASAs most expensive project?\n",
      "Answers:\n",
      "[   {   'answer': 'The Space Shuttle was the most expensive project in the '\n",
      "                  'history of NASA. It cost over $100 billion to build.'}]\n"
     ]
    }
   ],
   "source": [
    "result = pipe.run(\n",
    "    query=\"what was NASAs most expensive project?\",\n",
    "    params={\n",
    "        \"Retriever\": {\"top_k\": 3},\n",
    "        \"Generator\": {\"top_k\": 1}\n",
    "    }\n",
    ")\n",
    "\n",
    "print_answers(result, details=\"minimum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297,
     "referenced_widgets": [
      "6150fc9a812144ac9c310660029f1d75",
      "9c733591eca548b5ab094ce91050ac54",
      "a5a1f19208094925b58abb26eeaf91ea",
      "7171e40a5d2144c38a0301e84cf2eb53",
      "79bd20444439447688ee48b8c2f178dc",
      "19ced56ab647457faa547eac21ad310f",
      "e14d27f225914271a0d8753606d962a6",
      "995b6501ccb1402b9ca7dee037e110f6",
      "0f78cb865d5a4113925f1dd5519b9a57",
      "8f90ede1412741498e3798d0b019def1",
      "6c0c7d0d49d445b397416e550fc00d02"
     ]
    },
    "id": "ZznXSQy2YkYp",
    "outputId": "04a57296-3261-457a-e768-d7324d4f0e95"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f54a0d217f40f183d267a4ddc9d4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: tell me something interesting about the history of Earth?\n",
      "Answers:\n",
      "[   {   'answer': \"I don't know if this is what you're looking for, but I've \"\n",
      "                  \"always been fascinated by the fact that the Earth's \"\n",
      "                  'magnetic field is so weak compared to the rest of the solar '\n",
      "                  'system. The magnetic field of the Earth is about 1/10th the '\n",
      "                  'strength of that of the strongest magnetic field in the '\n",
      "                  'Solar System.'}]\n"
     ]
    }
   ],
   "source": [
    "result = pipe.run(\n",
    "        query=\"tell me something interesting about the history of Earth?\",\n",
    "        params={\n",
    "            \"Retriever\": {\"top_k\": 3},\n",
    "            \"Generator\": {\"top_k\": 1}\n",
    "        })\n",
    "\n",
    "print_answers(result, details=\"minimum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be3d20c12ccf4798aeb8650d0e1a07f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: who created the Nobel prize and why?\n",
      "Answers:\n",
      "[   {   'answer': 'Alfred Nobel was a Swedish industrialist and philanthropist '\n",
      "                  'who died in 1896. His will specified that his fortune be '\n",
      "                  'used to create a series of prizes for those who confer the '\n",
      "                  '\"greatest benefit on mankind\" in physics, chemistry, '\n",
      "                  'physiology or medicine, literature, and peace. The Nobel '\n",
      "                  \"Prize was funded by Alfred Nobel's personal fortune, 94% of \"\n",
      "                  'his fortune to the Nobel Foundation that now forms the '\n",
      "                  'economic base of the Nobel Prize.'}]\n"
     ]
    }
   ],
   "source": [
    "result = pipe.run(\n",
    "        query=\"who created the Nobel prize and why?\",\n",
    "        params={\n",
    "            \"Retriever\": {\"top_k\": 10},\n",
    "            \"Generator\": {\"top_k\": 1}\n",
    "        })\n",
    "\n",
    "print_answers(result, details=\"minimum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f8ce6577ac4c0ea988cb03baa6a46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: how is the nobel prize funded?\n",
      "Answers:\n",
      "[   {   'answer': 'The Nobel Prizes are awarded by the Swedish Academy of '\n",
      "                  'Sciences, which is a non-profit organization that was '\n",
      "                  'founded by Alfred Nobel in 1900. It is funded by his '\n",
      "                  'personal fortune, which now forms the economic base of the '\n",
      "                  'Nobel Prize.'}]\n"
     ]
    }
   ],
   "source": [
    "result = pipe.run(\n",
    "        query=\"how is the nobel prize funded?\",\n",
    "        params={\n",
    "            \"Retriever\": {\"top_k\": 10},\n",
    "            \"Generator\": {\"top_k\": 1}\n",
    "        })\n",
    "\n",
    "print_answers(result, details=\"minimum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "haystack_lfqa.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m91"
  },
  "interpreter": {
   "hash": "e81a84c338879f0412495ea98350e80595740634d3ce0fba8d30f35c60f1a4c3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
