{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bcf8e64",
   "metadata": {
    "id": "2bcf8e64"
   },
   "source": [
    "<!--<badge>--><a href=\"https://colab.research.google.com/github/pinecone-io/examples/blob/master/image_search/image_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><!--</badge>-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae2699",
   "metadata": {
    "id": "28ae2699"
   },
   "source": [
    "# Hybrid Image and Keyword Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e79d22c",
   "metadata": {
    "id": "8e79d22c",
    "papermill": {
     "duration": 0.058948,
     "end_time": "2021-04-28T22:35:28.242456",
     "exception": false,
     "start_time": "2021-04-28T22:35:28.183508",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f87b7",
   "metadata": {
    "id": "ed7f87b7"
   },
   "source": [
    "### What is Image Search and how will we use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-simulation",
   "metadata": {
    "id": "finished-simulation",
    "papermill": {
     "duration": 0.058948,
     "end_time": "2021-04-28T22:35:28.242456",
     "exception": false,
     "start_time": "2021-04-28T22:35:28.183508",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "One may find themselves with an image, looking for similar images among a large image corpus. The difficult part of this requirement is instantly retrieving, at scale, similar images, especially when there are tens of millions or billions of images from which to choose.\n",
    "\n",
    "In this example, we will walk you through the mechanics of how to solve this problem using an off-the-shelf, pretrained, neural network to generate data structures known as [vector embeddings](https://www.pinecone.io/learn/vector-embeddings/). We will use Pinecone's vector database offering to find images with similar vector embeddings to an _query image_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016a0805",
   "metadata": {
    "id": "016a0805"
   },
   "source": [
    "### Learning Goals and Estimated Reading Time\n",
    "\n",
    "_By the end of this 15 minute demo (on a recent MacBook Pro, or up to an hour on Google Colab), you will have:_\n",
    " 1. Learned about Pinecone's value for solving realtime image search requirements!\n",
    " 2. Stored and retrieved vectors from Pinecone your very-own Pinecone Vector Database.\n",
    " 3. Encoded images as vectors using a pretrained neural network (i.e. no model training necessary).\n",
    " 4. Queried Pinecone's Vector Database to find similar images to the query in question.\n",
    " \n",
    "Once all data is encoded as vectors, and is in your Pinecone Index, results of Pinecone queries are returned, on average, in tens of milliseconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc5b028",
   "metadata": {
    "id": "afc5b028"
   },
   "source": [
    "## Setup: Prerequisites and Image Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63155b7",
   "metadata": {
    "id": "f63155b7"
   },
   "source": [
    "### Python 3.7+\n",
    "\n",
    "This code has been tested with Python 3.7. It is recommended to run this code in a virtual environment or Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dd630e",
   "metadata": {
    "id": "00dd630e"
   },
   "source": [
    "### Acquiring your Pinecone API Key\n",
    "\n",
    "A Pinecone API key is required. You can obtain a complimentary key on our [our website](https://app.pinecone.io/). Either add `PINECONE_EXAMPLE_API_KEY` to your list of environmental variables, or manually enter it after running the below cell (a prompt will pop up requesting the API key, storing the result within this kernel (session))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258bdbd1",
   "metadata": {
    "id": "258bdbd1",
    "papermill": {
     "duration": 0.04542,
     "end_time": "2021-04-28T22:35:28.335525",
     "exception": false,
     "start_time": "2021-04-28T22:35:28.290105",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Installing and Importing Prerequisite Libraries:\n",
    "All prerequisites are installed and listed in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d1ae55",
   "metadata": {
    "id": "d4d1ae55"
   },
   "source": [
    "#### Installing via `pip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d322d91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d322d91",
    "outputId": "f9e8fd34-a71e-4c7a-86ff-78040810c850",
    "papermill": {
     "duration": 116.33689,
     "end_time": "2021-04-28T22:37:24.803401",
     "exception": false,
     "start_time": "2021-04-28T22:35:28.466511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "                 torchvision \\\n",
    "                 seaborn \\\n",
    "                 tqdm \\\n",
    "                 mmh3 \\\n",
    "                 nltk \\\n",
    "                 pycocotools \\\n",
    "                 transformers\n",
    "#                 #  pinecone-client \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f99999",
   "metadata": {},
   "source": [
    "Temporary - install pinecone's client from a side branch containing new hybrid API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tvkP7T5PO3SI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tvkP7T5PO3SI",
    "outputId": "a14e3018-d0ce-4db7-8a77-9450d498bce6"
   },
   "outputs": [],
   "source": [
    "!pip3 install -U git+https://github.com/pinecone-io/pinecone-python-client.git@add-hybridapi-wiring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19036494",
   "metadata": {
    "id": "19036494"
   },
   "source": [
    "#### Importing and Defining Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661c69f1",
   "metadata": {
    "id": "661c69f1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pinecone\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as f\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "DATA_DIRECTORY = 'tmp'\n",
    "INDEX_NAME = 'image-hybrid-search'\n",
    "INDEX_DIMENSION = 768\n",
    "BATCH_SIZE=100\n",
    "NUM_CAPTIONS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cab1fe",
   "metadata": {
    "id": "f0cab1fe"
   },
   "source": [
    "### Helper Module\n",
    "\n",
    "This helper module will be imported and will enable this notebook to be self-contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18724b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o helper.py https://raw.githubusercontent.com/pinecone-io/examples/ilai/hybrid_image_search/hybrid_image_search//helper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ms2zT7WZHiUY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "ms2zT7WZHiUY",
    "outputId": "eddd19f7-f0c6-4c9a-a0d0-43be87199e0d"
   },
   "outputs": [],
   "source": [
    "import helper as h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-colony",
   "metadata": {
    "id": "sitting-colony",
    "papermill": {
     "duration": 0.046053,
     "end_time": "2021-04-28T22:37:26.485100",
     "exception": false,
     "start_time": "2021-04-28T22:37:26.439047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Downloading Data\n",
    "\n",
    "To demonstrate image search using Pinecone, we will download 100,000 small images using [built-in datasets](https://pytorch.org/vision/stable/datasets.html) available with the `torchvision` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dV2QzCT4dkTT",
   "metadata": {
    "id": "dV2QzCT4dkTT"
   },
   "source": [
    "Get the COCO annotaions dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oJlLwLpiH0ds",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oJlLwLpiH0ds",
    "outputId": "bdf87970-a322-4763-f1dc-c35808f03408"
   },
   "outputs": [],
   "source": [
    "!wget -N http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "!wget -N http://images.cocodataset.org/zips/val2017.zip\n",
    "!wget -N http://images.cocodataset.org/zips/train2017.zip\n",
    "!unzip -qn train2017.zip\n",
    "!unzip -qn annotations_trainval2017.zip\n",
    "!unzip -qn val2017.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PiQwuF5LIuma",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PiQwuF5LIuma",
    "outputId": "3c88c574-485d-4722-ea0a-7f48e3101dbf"
   },
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.CocoCaptions('train2017/', 'annotations/captions_train2017.json', transform=h.preprocess, target_transform=lambda x: x[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9382a3d5-8f40-4d88-8f97-d35feade6097",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PiQwuF5LIuma",
    "outputId": "3c88c574-485d-4722-ea0a-7f48e3101dbf"
   },
   "outputs": [],
   "source": [
    "val_dataset = torchvision.datasets.CocoCaptions('val2017/', 'annotations/captions_val2017.json', transform=h.preprocess, target_transform=lambda x: x[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efff7f4",
   "metadata": {
    "id": "3efff7f4"
   },
   "source": [
    "### Inspecting Images\n",
    "These are some of the images from what was just downloaded. If interested, read about the COCO image dataset [here](https://cocodataset.org/#captions-2015)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sfPDmk8Pj0fO",
   "metadata": {
    "id": "sfPDmk8Pj0fO"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa52acdb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fa52acdb",
    "outputId": "2f6e57e6-bec3-42c8-c37b-b2d5756c6ec6"
   },
   "outputs": [],
   "source": [
    "h.show_random_images_from_full_dataset(dataset, num_rows=5, num_cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d44efe9",
   "metadata": {
    "id": "3d44efe9"
   },
   "source": [
    "## Generating Embeddings and Sending them to Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fa256c",
   "metadata": {
    "id": "76fa256c"
   },
   "source": [
    "### Loading a Pretrained Image Embedding Model\n",
    "\n",
    "We will use a pretrianed Vision Transformer model to generate image embedding vectors. \n",
    "This model will create a 768-dimensional sequence of floats for each input image. We will use this output as an embedding associated with an image.\n",
    "You can read more about ViT models [here](https://arxiv.org/abs/2010.11929)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582c647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "model = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78210b4",
   "metadata": {
    "id": "d78210b4"
   },
   "source": [
    "### On Comparing Embeddings\n",
    "\n",
    "Two embeddings might look like something like this:\n",
    "\n",
    "- \\[-0.02, 0.06, 0.0, 0.01, 0.08, -0.03, 0.01, 0.02, 0.01, 0.02, -0.07, -0.11, -0.01, 0.08, -0.04\\]\n",
    "- \\[-0.04, -0.09, 0.04, -0.1, -0.05, -0.01, -0.06, -0.04, -0.02, -0.04, -0.04, 0.07, 0.03, 0.02, 0.03\\]\n",
    "\n",
    "In order to determine how similar they are, we use a [simple](https://towardsdatascience.com/importance-of-distance-metrics-in-machine-learning-modelling-e51395ffe60d) formula that takes a very short time to compute. Similarity scores are, in general, an excellent proxy for image similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daa2327",
   "metadata": {
    "id": "6daa2327"
   },
   "source": [
    "### Creating Our Pinecone Index\n",
    "\n",
    "The process for creating a Pinecone Index requires your Pinecone API key, the name of your index, and the number of dimensions of each vector (1000).\n",
    "\n",
    "In this example, to compare embeddings, we will use the [cosine similarity score](https://en.wikipedia.org/wiki/Cosine_similarity) because this model generates un-normalized probability vectors. While this calculation is trivial when comparing two vectors, it will take quite a long time when needing to compare a query vector against millions or billions of vectors and determine those most similar with the query vector.\n",
    "\n",
    "Note: For the moment hybrid search only supports naive dot product, so we will need to normalize the vetors by the l2 norn to get the actual cosine distance metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1136b796",
   "metadata": {
    "id": "1136b796"
   },
   "source": [
    "### What is Pinecone for?\n",
    "\n",
    "There is often a technical requirement to compare one vector to tens or hundreds of millions or more vectors, to do so with low latency (less than 50ms) and a high throughput. Pinecone solves this problem with its managed vector database service, and we will demonstrate this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8qbwwSeBPtnh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "8qbwwSeBPtnh",
    "outputId": "3b986843-fc6c-48c2-9d02-11e1665aeac1"
   },
   "outputs": [],
   "source": [
    "pinecone.init(h.pinecone_api_key, environment='us-west1-gcp')\n",
    "\n",
    "metadata_config = {\n",
    "    \"indexed\": []\n",
    "}\n",
    "\n",
    "if INDEX_NAME not in pinecone.list_indexes():\n",
    "    pinecone.create_index(name=INDEX_NAME, dimension=INDEX_DIMENSION,\n",
    "                          metric=\"dotproduct\",\n",
    "                          pod_type=\"s1h\",\n",
    "                          index_config={\"hybrid_search\": {\"avg_doc_len\": 52}},\n",
    "                          metadata_config=metadata_config)\n",
    "\n",
    "index = pinecone.Index(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7AoSzbC6pn2m",
   "metadata": {
    "id": "7AoSzbC6pn2m"
   },
   "source": [
    "## Creating keywords sparse vectors\n",
    "\n",
    "The textual image captions are tokenzied indvidually, which will be used by pinecone's BM25 hybrid API to create sparse vector represnations of the keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rar_6_0Qpsdq",
   "metadata": {
    "id": "rar_6_0Qpsdq"
   },
   "outputs": [],
   "source": [
    "import mmh3\n",
    "import nltk\n",
    "from typing import Dict\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class WordEncoder():\n",
    "\n",
    "    def __init__(self):\n",
    "        nltk.download('punkt')\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "\n",
    "    def encode(self, text: str) -> Dict[int, int]:\n",
    "        words = [self.stemmer.stem(word) for word in word_tokenize(text)]\n",
    "        ids = [mmh3.hash(word, signed=False) for word in words]\n",
    "        return dict(Counter(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ADscgtdn22WO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ADscgtdn22WO",
    "outputId": "f25eabc7-82a4-4d5c-f74b-e7bbb64295be"
   },
   "outputs": [],
   "source": [
    "from collections import Iterable\n",
    "sparse_encoder = WordEncoder()\n",
    "\n",
    "def encode_image_captions(image_captions):\n",
    "    if isinstance(image_captions, Iterable):\n",
    "        image_captions = \". \".join(image_captions)\n",
    "    return sparse_encoder.encode(image_captions)\n",
    "\n",
    "def get_sparse_representation(captions):\n",
    "    return map(encode_image_captions, captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afd646d",
   "metadata": {
    "id": "2afd646d"
   },
   "source": [
    "### Preparing Vector Embeddings\n",
    "\n",
    "We will encode the downloaded images for upload to Pinecone, and store the associated class of each image as metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312d338a",
   "metadata": {
    "id": "312d338a"
   },
   "source": [
    "#### Creating Vector IDs\n",
    "Each vector ID will have a prefix corresponding to the dataset's name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77a0771",
   "metadata": {
    "id": "b77a0771"
   },
   "outputs": [],
   "source": [
    "def get_vector_ids(batch_number, batch_size, prefix):\n",
    "    \"\"\"Return vector ids.\"\"\"\n",
    "    start_index = batch_number * batch_size\n",
    "    end_index = start_index + batch_size\n",
    "    ids = np.arange(start_index, end_index)\n",
    "    ids_with_prefix = map(lambda x: f'{prefix}.{str(x)}', ids)\n",
    "    return ids_with_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3970e8d5",
   "metadata": {
    "id": "3970e8d5"
   },
   "source": [
    "#### Creating metadata for each vector containing class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IG110uNE5qhM",
   "metadata": {
    "id": "IG110uNE5qhM"
   },
   "outputs": [],
   "source": [
    "def get_vector_metadata(labels):\n",
    "    \"\"\"Return list of {'label': <class name>}.\"\"\"\n",
    "    get_cpation_sentences = lambda caps: {f'label_{i}': cap for i, cap in enumerate(caps)}\n",
    "    return map(get_cpation_sentences, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755dd63d",
   "metadata": {
    "id": "755dd63d"
   },
   "source": [
    "#### Constructing Vector Embeddings\n",
    "\n",
    "In a Pinecone Vector Database, there are three components to every Pinecone vector embedding:\n",
    "\n",
    " - a vector ID\n",
    " - a sequence of floats of a user-defined, fixed dimension\n",
    " - vector metadata (a key-value mapping, used for filtering at runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b0e37",
   "metadata": {
    "id": "b32b0e37"
   },
   "outputs": [],
   "source": [
    "def get_vectors_from_batch(preprocessed_data, captions, batch_number, dataset, normalize=True, hybrid=True):\n",
    "    \"\"\"Return list of tuples like (vector_id, vector_values, vector_metadata).\"\"\"\n",
    "    num_records = len(preprocessed_data)\n",
    "    prefix = dataset.__class__.__name__\n",
    "    with torch.no_grad():\n",
    "        # generate image embeddings with PyTorch model\n",
    "        preprocessed_data = preprocessed_data.to(device)\n",
    "        vector_values = model(preprocessed_data).pooler_output\n",
    "        if normalize:\n",
    "            vector_values = f.normalize(vector_values, p=2, dim=1)\n",
    "        vector_values = vector_values.cpu().tolist()\n",
    "\n",
    "    # return respective IDs/metadata for each image embedding\n",
    "    vector_ids = get_vector_ids(batch_number, num_records, prefix)\n",
    "    if hybrid:\n",
    "        vector_metadata = [{}] * num_records\n",
    "        sparse_rep = get_sparse_representation(captions)\n",
    "        return list(zip(vector_ids, vector_values, sparse_rep, vector_metadata))\n",
    "    else:\n",
    "        vector_metadata = get_vector_metadata(captions)\n",
    "        return list(zip(vector_ids, vector_values, vector_metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374d3a7d",
   "metadata": {
    "id": "374d3a7d"
   },
   "source": [
    "#### Upsert Vectors to Pinecone\n",
    "This function iterates through a dataset in batches, generates a list of vector embeddings (as in the the above example) and upserts in batches to Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f637a5",
   "metadata": {
    "id": "d9f637a5"
   },
   "outputs": [],
   "source": [
    "def upsert_image_embeddings(dataset, pinecone_index, batch_size=BATCH_SIZE, num_rows=np.inf, hybrid=True):\n",
    "    \"\"\"Iterate through dataset, generate embeddings and upsert in batches to Pinecone index.\n",
    "    \n",
    "    Args:\n",
    "     - dataset: a PyTorch Dataset\n",
    "     - pinecone_index: your Pinecone index\n",
    "     - batch_size: batch size\n",
    "     - num_rows: Number of initial rows to use of dataset, use all rows if None. \n",
    "    \"\"\"\n",
    "    if num_rows < np.inf:\n",
    "        if num_rows > len(dataset):\n",
    "            raise ValueError(f'`num_rows` should not exceed length of dataset: {len(dataset)}')\n",
    "        sampler = range(num_rows)\n",
    "    else:\n",
    "        sampler = None\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=sampler, num_workers=8)\n",
    "    tqdm_kwargs = h.get_tqdm_kwargs(dataloader, num_rows=num_rows)\n",
    "    for batch_number, (images, captions) in tqdm(enumerate(dataloader), **tqdm_kwargs):\n",
    "        captions = list(zip(*captions))\n",
    "        vectors = get_vectors_from_batch(\n",
    "            images, \n",
    "            captions, \n",
    "            batch_number, \n",
    "            dataloader.dataset,\n",
    "            normalize=hybrid,\n",
    "            hybrid=hybrid)\n",
    "        \n",
    "        if hybrid:\n",
    "            pinecone_index.hybrid.upsert(vectors)\n",
    "        else:\n",
    "            pinecone_index.upsert(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c655647a",
   "metadata": {
    "id": "c655647a"
   },
   "source": [
    "### Begin Upsert for all 100,000 Images\n",
    "One progress bar is generated per dataset. Truncate number of rows in each dataset by modifying `num_rows` parameter value in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf5703f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "77cb566e18bd47779ac1dbdf9e7bd164",
      "0484ae4eaf1b4b948816ceb7c78a8882",
      "e86ea32ac6c3435dba7f58eb949441c9",
      "8d3a55e488734365aeeaa42133ed4450",
      "2ce0248776d04a90b38ea9b1ee7e3078",
      "25cfbfa28b434e9da14f1fe94b618db5",
      "613d02784e994437be6640ec639ec1ec",
      "d1632569ad4d44afb511bc7efe7c5fd7",
      "ebc73d1de95a48438d3049d704b0629e",
      "81bdfe196576427abc5f398ff39dc5a4",
      "09ad6506c897490992b31229a997de2b"
     ]
    },
    "id": "daf5703f",
    "outputId": "9e9b9d74-505a-4986-8480-35d5a5020ff9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "upsert_image_embeddings(dataset, index, num_rows=100_000, batch_size=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad14d362",
   "metadata": {
    "id": "ad14d362"
   },
   "source": [
    "### View Progress On The [Pinecone Console](https://app.pinecone.io) (sample screenshot below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-driving",
   "metadata": {
    "id": "behind-driving",
    "papermill": {
     "duration": 0.099945,
     "end_time": "2021-04-28T22:39:54.703643",
     "exception": false,
     "start_time": "2021-04-28T22:39:54.603698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Querying Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-notice",
   "metadata": {
    "id": "lonely-notice",
    "papermill": {
     "duration": 0.103076,
     "end_time": "2021-04-28T22:39:54.902705",
     "exception": false,
     "start_time": "2021-04-28T22:39:54.799629",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now that all the embeddings of the images are on Pinecone's database, it's time to demonstrate Pinecone's lightning fast query capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3300e81",
   "metadata": {
    "id": "a3300e81"
   },
   "source": [
    "###  Pinecone Example Usage\n",
    "\n",
    "In the below example we query Pinecone's API with an embedding of a query image to return the vector embeddings that have the highest similarity score. Pinecone effeciently estimates which of the uploaded vector embeddings have the highest similarity when paired with the query term's embedding, and the database will scale to billions of embeddings maintaining low-latency and high throughput. In this example we have upserted 100,000 embeddings. Our starter plan supports up to one million."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f2e985",
   "metadata": {
    "id": "58f2e985"
   },
   "source": [
    "#### Example: Pinecone API Request and Response\n",
    "\n",
    "Let's find images similar to the `query_image` variable, shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d02b6a1",
   "metadata": {
    "id": "2d02b6a1"
   },
   "source": [
    "#### Example Query Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VNoBDcEiEDb4",
   "metadata": {
    "id": "VNoBDcEiEDb4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_hybrid_index(query_image, query_captions=[\"\"], alpha = 0.5, normalize=True):\n",
    "    with torch.no_grad():\n",
    "        vector_values = model(query_image.unsqueeze(0).to(device)).pooler_output\n",
    "        if normalize:\n",
    "            vector_values = f.normalize(vector_values, p=2, dim=1)\n",
    "        query_embedding = vector_values.cpu().tolist()\n",
    "\n",
    "    query_sparse_rep = encode_image_captions(query_captions)\n",
    "    response = index.hybrid.query(vector=query_embedding,\n",
    "                                sparse_vector = query_sparse_rep,\n",
    "                                top_k=4, \n",
    "                                alpha=alpha,\n",
    "                                include_metadata=False)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4266a52e-b3ad-4082-8b54-b07c62883d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, len(val_dataset))\n",
    "query_image, query_captions = val_dataset[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5511a6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.show_query_image(query_image, query_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a08033",
   "metadata": {
    "id": "e2a08033"
   },
   "source": [
    "#### Enriched Response\n",
    "In the next few lines, we look up the actual images associated to the vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tepej1lzmqHj",
   "metadata": {
    "id": "tepej1lzmqHj"
   },
   "outputs": [],
   "source": [
    "def plot_result(query_image, query_captions, alpha=0.5):\n",
    "    response = query_hybrid_index(query_image, query_captions, alpha=alpha)\n",
    "    h.show_response_as_grid(response, dataset, figsize=(8, 8), nrows=2, num_captions=1, wrap_len=40,)\n",
    "    fig = plt.gcf()\n",
    "    fig.text(0.5, .97, f\"Alpha: {alpha}\", horizontalalignment='center', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f439bb3-bd04-4a4f-bdb2-02ccf7875eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(query_image, query_captions, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb6ec54",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Let's take one particular image, and explore how different keywords in the query affect the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58d49d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4501 # woman playing tennis\n",
    "query_image, query_captions = val_dataset[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c41c61-cfab-47b1-b4d4-f2385990295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_query_image(query_image, query_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9367f47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [[\"\"], [\"woman\"], [\"yellow\"]]\n",
    "fig, axes = plt.subplots(4, len(keywords), figsize=(12, 16))\n",
    "for i, query_captions in enumerate(keywords):\n",
    "    response = query_hybrid_index(query_image, query_captions)\n",
    "    h.show_response_as_grid(response, dataset, num_captions=1, wrap_len=36, axes=axes[:, i])\n",
    "    fig.text(0.25 * (i + 1), .92, f\"Keyowrd: {query_captions[0]}\", horizontalalignment='center', fontsize=16)\n",
    "\n",
    "line = plt.Line2D((.37,.37),(.1,.93), color=\"k\", linewidth=3)\n",
    "fig.add_artist(line)    \n",
    "line = plt.Line2D((.65,.65),(.1,.93), color=\"k\", linewidth=3)\n",
    "fig.add_artist(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cae2f0f",
   "metadata": {},
   "source": [
    "Also, let's explore how the hybrid search result differ from sparse search or dense search only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8558d718-1356-4cbc-80a7-c2022e22daa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_values = [0, 0.5, 1]\n",
    "query_captions = [\"woman playing tennis\"]\n",
    "fig, axes = plt.subplots(4, len(alpha_values), figsize=(12, 16))\n",
    "for i, alpha in enumerate(alpha_values):\n",
    "    response = query_hybrid_index(query_image, query_captions, alpha=alpha)\n",
    "    h.show_response_as_grid(response, dataset, num_captions=1, wrap_len=36, axes=axes[:, i])\n",
    "    fig.text(0.25 * (i + 1), .92, f\"Alpha: {alpha}\", horizontalalignment='center', fontsize=16)\n",
    "\n",
    "line = plt.Line2D((.37,.37),(.1,.93), color=\"k\", linewidth=3)\n",
    "fig.add_artist(line)    \n",
    "line = plt.Line2D((.65,.65),(.1,.93), color=\"k\", linewidth=3)\n",
    "fig.add_artist(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6323a6",
   "metadata": {
    "id": "da6323a6"
   },
   "source": [
    "#### Results\n",
    "\n",
    "We invite the reader to explore various queries to see how they come up. In the one above, we chose one of the CIFAR-10 images as the query image. Note that the query image embedding need not exist in your Pinecone index in order to find similar images. Additionally, the search results are only as good as the embeddings, which are based on the quality and quantity of the images as well as how expressive the model used is. There are plenty of other out of the box, pretrained models in PyTorch and elsewhere!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-pennsylvania",
   "metadata": {
    "id": "sealed-pennsylvania",
    "papermill": {
     "duration": 0.144059,
     "end_time": "2021-04-28T22:40:07.144120",
     "exception": false,
     "start_time": "2021-04-28T22:40:07.000061",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Pinecone Example Usage with Metadata\n",
    "\n",
    "Extensive predicate logic can be applied to metadata filtering, just like the [WHERE clause](https://www.pinecone.io/learn/vector-search-filtering/) in SQL! Pinecone's [metadata feature](https://www.pinecone.io/docs/metadata-filtering/) provides easy-to-implement filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dec61b1",
   "metadata": {
    "id": "4dec61b1"
   },
   "source": [
    "#### Example using Metadata\n",
    "\n",
    "For demonstration, let's use metadata to find all images classified as a _seal_ that look like the `query_image` variable shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f6b724",
   "metadata": {
    "id": "28f6b724"
   },
   "outputs": [],
   "source": [
    "response = index.query(\n",
    "    query_embedding, \n",
    "    top_k=25, \n",
    "    filter={\"label\": {\"$eq\": \"seal\"}},\n",
    "    include_metadata=True\n",
    ")\n",
    "h.show_response_as_grid(response, datasets, 5, 5, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a1f8f2",
   "metadata": {
    "id": "11a1f8f2"
   },
   "source": [
    "#### Results\n",
    "\n",
    "All of the results returned are indeed seals, and many of them do look like the query image! Note how the cosine similarity scores are returned in descending order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f158c431",
   "metadata": {
    "id": "f158c431"
   },
   "source": [
    "#### Additional Note On Querying Pinecone\n",
    "\n",
    "In this example, you queried your Pinecone index with an embedding that was already in the index, however that is not necessary at all. For this index, _any 1000-dimensional embedding_ can be used to query Pinecone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ed7fab",
   "metadata": {
    "id": "d1ed7fab"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this example, we demonstrated how Pinecone makes it possible to do realtime image similarity search using a pre-trained computer vision model! We also demonstrated the use of metadata filtering with querying Pinecone's vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa2789b",
   "metadata": {
    "id": "1fa2789b"
   },
   "source": [
    "### Like what you see? Explore our [community](https://www.pinecone.io/community/) \n",
    "\n",
    "Learn more about semantic search and the rich, performant, and production-level feature set of Pinecone's Vector Database by visiting https://pinecone.io, connecting with us [here](https://www.pinecone.io/contact/) and following us on [LinkedIn](https://www.linkedin.com/company/pinecone-io). If interested in some of the algorithms that allow for effecient estimation of similar vectors, visit our Algorithms and Libraries section of our [Learning Center](https://www.pinecone.io/learn/)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "58f2e985",
    "da6323a6",
    "4dec61b1",
    "11a1f8f2",
    "f158c431"
   ],
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m100"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 295.210786,
   "end_time": "2021-04-28T22:40:22.227918",
   "environment_variables": {},
   "exception": null,
   "input_path": "/notebooks/image_search/simple_pytorch_image_search.ipynb",
   "output_path": "/notebooks/tmp/image_search/simple_pytorch_image_search.ipynb",
   "parameters": {},
   "start_time": "2021-04-28T22:35:27.017132",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0484ae4eaf1b4b948816ceb7c78a8882": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25cfbfa28b434e9da14f1fe94b618db5",
      "placeholder": "​",
      "style": "IPY_MODEL_613d02784e994437be6640ec639ec1ec",
      "value": " 60%"
     }
    },
    "0950b0d6406f43c28cf91c14df95c6dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7bb53bfafb934a709ee2973e9ecb7bae",
      "placeholder": "​",
      "style": "IPY_MODEL_b60a74d557404e34ab55c474c1e14abc",
      "value": "100%"
     }
    },
    "09ad6506c897490992b31229a997de2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "15e1a556bb744585aa5a1a742e4ecf76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2179aafe07fb44fc8d9c428f64eb5a3c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25cfbfa28b434e9da14f1fe94b618db5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ce0248776d04a90b38ea9b1ee7e3078": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "459f2c85c9d34449b459be66f8cd159b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f6bfc8419b7b4260ab44de57e6bcf224",
      "max": 4958839,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ff06322793614dcea84cd0467ae1dee3",
      "value": 4958839
     }
    },
    "5054f46835bc4857b8a26e5f6e576eae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2179aafe07fb44fc8d9c428f64eb5a3c",
      "placeholder": "​",
      "style": "IPY_MODEL_617735afa07a442d9c32a0b1182028ac",
      "value": " 4.73M/4.73M [00:00&lt;00:00, 23.7MB/s]"
     }
    },
    "54160ba3436b448188671f88ed34a17a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_76b2ca44b34f40f687fc78c3fb86db26",
      "placeholder": "​",
      "style": "IPY_MODEL_97d94d3eb85c450b8bbc344eab7094f2",
      "value": " 15/25 [02:48&lt;01:52, 11.24s/chunk of 200 CocoCaptions vectors]"
     }
    },
    "568808a30c4a4d1486c31f8db76cac8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_612e935deaa04548a991d217452d25cc",
      "max": 25,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6d59d72b10f2499eb4174c7c3d42d343",
      "value": 15
     }
    },
    "612e935deaa04548a991d217452d25cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "613d02784e994437be6640ec639ec1ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "617735afa07a442d9c32a0b1182028ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6d59d72b10f2499eb4174c7c3d42d343": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "76b2ca44b34f40f687fc78c3fb86db26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77cb566e18bd47779ac1dbdf9e7bd164": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0484ae4eaf1b4b948816ceb7c78a8882",
       "IPY_MODEL_e86ea32ac6c3435dba7f58eb949441c9",
       "IPY_MODEL_8d3a55e488734365aeeaa42133ed4450"
      ],
      "layout": "IPY_MODEL_2ce0248776d04a90b38ea9b1ee7e3078"
     }
    },
    "7bb53bfafb934a709ee2973e9ecb7bae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81bdfe196576427abc5f398ff39dc5a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d148a739f514f0784160979df0f197a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d3a55e488734365aeeaa42133ed4450": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_81bdfe196576427abc5f398ff39dc5a4",
      "placeholder": "​",
      "style": "IPY_MODEL_09ad6506c897490992b31229a997de2b",
      "value": " 15/25 [02:50&lt;01:53, 11.38s/chunk of 200 CocoCaptions vectors]"
     }
    },
    "97d94d3eb85c450b8bbc344eab7094f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b60a74d557404e34ab55c474c1e14abc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf1bdbbabe8342be81734eebcf0d7dc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0950b0d6406f43c28cf91c14df95c6dd",
       "IPY_MODEL_459f2c85c9d34449b459be66f8cd159b",
       "IPY_MODEL_5054f46835bc4857b8a26e5f6e576eae"
      ],
      "layout": "IPY_MODEL_c207e44e1ef44885a4f7beecfb0e1e8d"
     }
    },
    "c207e44e1ef44885a4f7beecfb0e1e8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce1bde0dae67478b8cbefd6aef697b31": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1632569ad4d44afb511bc7efe7c5fd7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d689434bc7dc4c058e4faf42d19f6a6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d77141cee45041e0bb4adfa82768dfbf",
       "IPY_MODEL_568808a30c4a4d1486c31f8db76cac8f",
       "IPY_MODEL_54160ba3436b448188671f88ed34a17a"
      ],
      "layout": "IPY_MODEL_ce1bde0dae67478b8cbefd6aef697b31"
     }
    },
    "d77141cee45041e0bb4adfa82768dfbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d148a739f514f0784160979df0f197a",
      "placeholder": "​",
      "style": "IPY_MODEL_15e1a556bb744585aa5a1a742e4ecf76",
      "value": " 60%"
     }
    },
    "e86ea32ac6c3435dba7f58eb949441c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d1632569ad4d44afb511bc7efe7c5fd7",
      "max": 25,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ebc73d1de95a48438d3049d704b0629e",
      "value": 15
     }
    },
    "ebc73d1de95a48438d3049d704b0629e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f6bfc8419b7b4260ab44de57e6bcf224": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff06322793614dcea84cd0467ae1dee3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
