{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/sparse/splade/splade-queries.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/sparse/splade/splade-queries.ipynb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse-Dense Vector Search with SPLADE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "SPLADE is a class of models that produce sparse embeddings. Unlike dense embeddings which can be difficult to interpret sparse embeddings map to tokens for easier interpretability. SPLADE models have been shown to consistently outperform dense models, particularly in out-of-domain settings.\n",
    "\n",
    "The following guide will show you how to construct SPLADE embeddings to use with Pinecone's sparse-dense index. See the [companion guide](https://github.com/pinecone-io/examples/blob/master/sparse/splade/splade-embedding-generation.ipynb) to learn how to generate embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "          git+https://git@github.com/pinecone-io/pinecone-python-client.git#egg=pinecone-client[grpc] \\\n",
    "          polars \\\n",
    "          transformers \\\n",
    "          torch \\\n",
    "          sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quora Dataset\n",
    "\n",
    "Load the popular Quora dataset with embeddings precomputed using\n",
    "\n",
    "* Dense: [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)\n",
    "* Sparse: [naver/splade-cocondenser-ensembledistil](https://huggingface.co/naver/splade-cocondenser-ensembledistil)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('https://storage.googleapis.com/gareth-pinecone-datasets/quora_all-MiniLM-L6-splade_v2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "pinecone.init(api_key=\"YOUR_API_KEY\", environment=\"YOUR_ENVIRONMENT\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"splade-embedding-query\"\n",
    "batch_size = 300\n",
    "dimension = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.create_index(\n",
    "    index_name,\n",
    "    pod_type='s1',\n",
    "    metric='dotproduct',\n",
    "    dimension=dimension,\n",
    "    metadata_config={\"indexed\": []}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pinecone import GRPCVector, GRPCSparseValues\n",
    "from google.protobuf.struct_pb2 import Struct\n",
    "\n",
    "with pinecone.GRPCIndex(index_name) as index:\n",
    "    for i in tqdm(range(0, len(df), batch_size)):\n",
    "        batch = df[i:min(i+batch_size, len(df))].to_dict(orient='records')\n",
    "        upserts = []\n",
    "        for row in batch:\n",
    "            metadata = Struct()\n",
    "            metadata.update(dict(text=row['text']))\n",
    "            u = GRPCVector(\n",
    "                id=str(row['id']),\n",
    "                values=row['values'].tolist(),\n",
    "                metadata=metadata,\n",
    "                sparse_values=GRPCSparseValues(\n",
    "                    indices=row['sparse_values']['indices'].tolist(),\n",
    "                    values=row['sparse_values']['values'].tolist()\n",
    "                )\n",
    "            )\n",
    "            upserts.append(u)\n",
    "        index.upsert(vectors=upserts, async_req=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse-Dense Queries with SPLADE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "\n",
    "class SPLADE:\n",
    "    def __init__(self, model):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model)\n",
    "\n",
    "    def __call__(self, text: str):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**inputs).logits\n",
    "\n",
    "        inter = torch.log1p(torch.relu(logits[0]))\n",
    "        token_max = torch.max(inter, dim=0)  # sum over input tokens\n",
    "        nz_tokens = torch.where(token_max.values > 0)[0]\n",
    "        nz_weights = token_max.values[nz_tokens]\n",
    "\n",
    "        order = torch.sort(nz_weights, descending=True)\n",
    "        nz_weights = nz_weights[order[1]]\n",
    "        nz_tokens = nz_tokens[order[1]]\n",
    "        return {'indices': nz_tokens.numpy().tolist(), 'values': nz_weights.numpy().tolist()}\n",
    "\n",
    "def hybrid_score_norm(dense, sparse, alpha: float):\n",
    "    \"\"\"Hybrid score using a convex combination\n",
    "    \"\"\"\n",
    "    if alpha < 0 or alpha > 1:\n",
    "        raise ValueError(\"Alpha must be between 0 and 1\")\n",
    "    hs = {\n",
    "        'indices': sparse['indices'],\n",
    "        'values':  [v * (1 - alpha) for v in sparse['values']]\n",
    "    }\n",
    "    return [v * alpha for v in dense], hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    'sentence-transformers/all-MiniLM-L6-v2',\n",
    "    device=device\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splade = SPLADE(\"naver/splade-cocondenser-ensembledistil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import SparseValues\n",
    "\n",
    "index = pinecone.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"nyc bites\"\n",
    "sparse = splade(text)\n",
    "dense = model.encode(text).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Only Sparse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdense, hsparse = hybrid_score_norm(dense, sparse, 0.0)\n",
    "index.query(top_k=3, vector=hdense, sparse_vector=SparseValues(**hsparse), include_metadata=True)['matches']"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hybrid"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hdense, hsparse = hybrid_score_norm(dense, sparse, 0.25)\n",
    "index.query(top_k=3, vector=hdense, sparse_vector=SparseValues(**hsparse), include_metadata=True)['matches']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hdense, hsparse = hybrid_score_norm(dense, sparse, 0.85)\n",
    "index.query(top_k=3, vector=hdense, sparse_vector=SparseValues(**hsparse), include_metadata=True)['matches']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Only Dense"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hdense, hsparse = hybrid_score_norm(dense, sparse, 1.0)\n",
    "index.query(top_k=3, vector=hdense, sparse_vector=SparseValues(**hsparse), include_metadata=True)['matches']"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
